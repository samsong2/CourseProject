8-1-syntagmatic-relation-discovery-entropy
https://d18ky98rnyall9.cloudfront.net/CS-V-2014-10_TM-9_Entropy_V2.4fd0ad50139c11e5bd8f39c4c6580840/full/540p/index.webm?Expires=1637452800&Signature=lNBAgxt8DbJV4YwlMgi-mHDvhHKXAGiD9CQN9ZHslshA2-npgwcaF~lHEGRR4koWo8TsL6TjvSDu0Nj3ZppRpgeOcm~A6K-Q6fyZubNMNxnKQ9Ne2MzgzwkHGwBAMf9xUD5LuxLZrD~LjYWkLDMCHuxo5nOtl~xmZrbMGYbWjKQ_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
0:00 : [SOUND]. This lecture is about the syntagmatic relation discovery, and entropy. In this lecture, we're going to continue talking about word association mining. In particular, we're going to talk about how to discover syntagmatic relations. And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. 
0:32 : By definition, syntagmatic relations hold between words that have correlated co-occurrences. That means, when we see one word occurs in context, we tend to see the occurrence of the other word. 
0:48 : So, take a more specific example, here. We can ask the question, whenever eats occurs, what other words also tend to occur? 
1:01 : Looking at the sentences on the left, we see some words that might occur together with eats, like cat, dog, or fish is right. But if I take them out and if you look at the right side where we only show eats and some other words, the question then is. Can you predict what other words occur to the left or to the right? 
1:28 : Right so this would force us to think about what other words are associated with eats. If they are associated with eats, they tend to occur in the context of eats. 
1:38 : More specifically our prediction problem is to take any text segment which can be a sentence, a paragraph, or a document. And then ask I the question, is a particular word present or absent in this segment? 
1:54 : Right here we ask about the word W. Is W present or absent in this segment? 
2:02 : Now what's interesting is that some words are actually easier to predict than other words. 
2:10 : If you take a look at the three words shown here, meat, the, and unicorn, which one do you think is easier to predict? 
2:20 : Now if you think about it for a moment you might conclude that 
2:24 : the is easier to predict because it tends to occur everywhere. So I can just say, well that would be in the sentence. 
2:31 : Unicorn is also relatively easy because unicorn is rare, is very rare. And I can bet that it doesn't occur in this sentence. 
2:42 : But meat is somewhere in between in terms of frequency. And it makes it harder to predict because it's possible that it occurs in a sentence or the segment, more accurately. 
2:53 : But it may also not occur in the sentence, so now let's study this problem more formally. 
3:02 : So the problem can be formally defined as predicting the value of a binary random variable. Here we denote it by X sub w, w denotes a word, so this random variable is associated with precisely one word. 
3:18 : When the value of the variable is 1, it means this word is present. When it's 0, it means the word is absent. And naturally, the probabilities for 1 and 0 should sum to 1, because a word is either present or absent in a segment. 
3:35 : There's no other choice. 
3:38 : So the intuition with this concept earlier can be formally stated as follows. The more random this random variable is, the more difficult the prediction will be. 
3:49 : Now the question is how does one quantitatively measure the randomness of a random variable like X sub w? 
3:56 : How in general, can we quantify the randomness of a variable and that's why we need a measure called entropy and this measure introduced in information theory to measure the randomness of X. There is also some connection with information here but that is beyond the scope of this course. 
4:17 : So for our purpose we just treat entropy function as a function defined on a random variable. In this case, it is a binary random variable, although the definition can be easily generalized for a random variable with multiple values. 
4:32 : Now the function form looks like this, there's the sum of all the possible values for this random variable. Inside the sum for each value we have a product of the probability 
4:45 : that the random variable equals this value and log of this probability. 
4:53 : And note that there is also a negative sign there. 
4:56 : Now entropy in general is non-negative. And that can be mathematically proved. 
5:02 : So if we expand this sum, we'll see that the equation looks like the second one. Where I explicitly plugged in the two values, 0 and 1. And sometimes when we have 0 log of 0, we would generally define that as 0, because log of 0 is undefined. 
5:28 : So this is the entropy function. And this function will give a different value for different distributions of this random variable. 
5:37 : And it clearly depends on the probability that the random variable taking value of 1 or 0. If we plot this function against the probability that the random variable is equal to 1. 
5:56 : And then the function looks like this. 
6:01 : At the two ends, that means when the probability of X 
6:07 : equals 1 is very small or very large, then the entropy function has a low value. When it's 0.5 in the middle then it reaches the maximum. 
6:20 : Now if we plot the function against the probability that X 
6:25 : is taking a value of 0 and the function would show exactly the same curve here, and you can imagine why. And so that's because 
6:42 : the two probabilities are symmetric, and completely symmetric. 
6:48 : So an interesting question you can think about in general is for what kind of X does entropy reach maximum or minimum. And we can in particular think about some special cases. For example, in one case, we might have a random variable that 
7:08 : always takes a value of 1. The probability is 1. 
7:16 : Or there's a random variable that 
7:19 : is equally likely taking a value of one or zero. So in this case the probability that X equals 1 is 0.5. 
7:30 : Now which one has a higher entropy? 
7:34 : It's easier to look at the problem by thinking of a simple example 
7:40 : using coin tossing. 
7:43 : So when we think about random experiments like tossing a coin, 
7:48 : it gives us a random variable, that can represent the result. It can be head or tail. So we can define a random variable X sub coin, so that it's 1 when the coin shows up as head, it's 0 when the coin shows up as tail. 
8:09 : So now we can compute the entropy of this random variable. And this entropy indicates how difficult it is to predict the outcome 
8:22 : of a coin toss. 
8:25 : So we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head or tail equally likely. So the two probabilities would be a half. Right? So both are equal to one half. 
8:44 : Another extreme case is completely biased coin, where the coin always shows up as heads. So it's a completely biased coin. 
8:54 : Now let's think about the entropies in the two cases. And if you plug in these values you can see the entropies would be as follows. For a fair coin we see the entropy reaches its maximum, that's 1. 
9:11 : For the completely biased coin, we see it's 0. And that intuitively makes a lot of sense. Because a fair coin is most difficult to predict. 
9:22 : Whereas a completely biased coin is very easy to predict. We can always say, well, it's a head. Because it is a head all the time. So they can be shown on the curve as follows. So the fair coin corresponds to the middle point where it's very uncertain. The completely biased coin corresponds to the end point where we have a probability of 1.0 and the entropy is 0. So, now let's see how we can use entropy for word prediction. Let's think about our problem is to predict whether W is present or absent in this segment. Again, think about the three words, particularly think about their entropies. 
10:06 : Now we can assume high entropy words are harder to predict. 
10:11 : And so we now have a quantitative way to tell us which word is harder to predict. 
10:20 : Now if you look at the three words meat, the, unicorn, again, and we clearly would expect meat to have a higher entropy than the unicorn. In fact if you look at the entropy of the, it's close to zero. Because it occurs everywhere. So it's like a completely biased coin. 
10:44 : Therefore the entropy is zero. 
10:48 : [MUSIC] 
