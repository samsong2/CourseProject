lesson-6-1-learning-to-rank-part-1-optional
https://d18ky98rnyall9.cloudfront.net/CS-V-2014-7_29_LearningtoRank_Part_1_CSRA_Final.efbb6000de4d11e49ff32711c51d6f19/full/540p/index.webm?Expires=1639267200&Signature=jGwYgBCdS~6coboZaYcd5WOp58ScYbxyUs05sAd41XcQcxoBA2sdkDSQetwRx~LrVVr~juRxDvsANUTOaNkGCw6RsdFmU~cXJC~3TQosaC~P35BeYrXbP0K3sV-r3MqWCS6fUIECVZSGC5mxM~X0e-ETLxwDT9KZN8N4SNPQWAs_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
0:00 : [MUSIC] 
0:07 : This lecture is about the Learning to Rank. In this lecture, we are going to continue talking about web search. In particular we're going to talk about the using machine learning to combine different features to improve the ranking function. 
0:22 : So the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results? In the previous lectures we have talked about a number of ways to rank documents. We have talked about some retrieval models like a BM25 or Query Light Code. They can generate a based this course for matching documents with a query. And we also talked about the link based approaches like page rank 
0:59 : that can give additional scores to help us improve ranking. 
1:03 : Now the question now is, how can we combine all these features and potentially many other features to do ranking? And this will be very useful for ranking webpages, not only just to improve accuracy, but also to improve the robustness of the ranking function. So that it's not easy for a spammer to just perturb a one or a few features to promote a page. 
1:27 : So the general idea of learning to rank is to use machine learning to combine this features to optimize the weights on different features to generate the optimal ranking function. 
1:40 : So we will assume that the given a query document pair Q and D, we can define a number of features. And these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as BM25 or Query Light Hold of punitive commands from a machine or PL2 etcetera. It can also be a link based score like or page rank score like. It can be also application of retrieval models to the ink text of the page. Those are the types of descriptions of links that point to this page. 
2:29 : So, these can all the clues whether this document is relevant, or not. 
2:35 : We can even include a feature such as whether the URL has a tilde because this might be indicator of home page or entry page. 
2:48 : So all these features can then be combined together to generate a ranking function. The question is, of course. How can we combine them? In this approach, we simply hypothesize that the probability that this document isn't relevant to this query is a function of all these features. So we can hypothesize this 
3:11 : that the probability of relevance is related to these features through a particular form of the function that has some parameters. These parameters can control the influence of different features of the final relevance. Now this is of course just an assumption. Whether this assumption really makes sense is a big question and that's they have to empirically evaluate the function. 
3:45 : But by hypothesizing that the relevance is related to these features in the particular way, we can then combine these features to generate the potential more powerful ranking function, a more robust ranking function. Naturally the next question is how do we estimate those parameters? How do we know which features should have a higher weight, and which features will have lower weight? So this is the task of training or learning, so in this approach what we will do is use some training data. Those are the data that have been charted by users so that we already know the relevant judgments. We already know which documents should be ranked high for which queries. And this information can be based on real judgments by users or this can also be approximated by just using click through information, where we can assume the clicked documents are better than the skipped documents clicked documents are relevant and the skipped documents are non-relevant. So in general with the fit such hypothesize ranking function to the training data meaning that we will try to optimize it's retrieval accuracy on the training data. And we can adjust these parameters to see 
5:09 : how we can optimize the performance of the functioning on the training data 
5:16 : in terms of some measures such as MAP or NDCG. 
5:20 : So the training date would look like a table of tuples. Each tuple has three elements, the query, the document, and the judgement. So it looks very much like our relevance judgement that we talked about in the evaluation of retrieval systems. [MUSIC] 
