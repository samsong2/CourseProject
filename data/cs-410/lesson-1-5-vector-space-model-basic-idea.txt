lesson-1-5-vector-space-model-basic-idea
https://d18ky98rnyall9.cloudfront.net/CS-V-2014-7_Vector_Space_Basic_CSRA_Final_v2.a35e9340cf3811e4b680ed55d157d6c0/full/540p/index.webm?Expires=1639440000&Signature=dz~UgEJ2NaUFEdDdBv60ys2DWvhbEs7LMKi9AlzUPcCvRrjkrfMg9Ad1p7smRt0Fkv7xqHlfnbd6cGIvP53WRvn7VQHSKEdk9ySFB9TJQwUDlHMWxaxHFYVvFMNgQhPXIr3zFoetdQtZYBXy6plONL8QaKzczjOTTCMbz~BJhB4_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
0:00 : [SOUND] This lecture is about the vector space retrieval model. We're going to give an introduction to its basic idea. 
0:18 : In the last lecture, we talked about the different ways of designing a retrieval model, which would give us a different arranging function. 
0:30 : In this lecture, we're going to talk about a specific way of designing a ramping function called a vector space retrieval model. 
0:37 : And we're going to give a brief introduction to the basic idea. 
0:44 : Vector space model is a special case of similarity based models as we discussed before. Which means we assume relevance is roughly similarity, between the document and the query. 
1:02 : Now whether is this assumption is true is actually a question. But in order to solve the search problem, we have to convert the vague notion of relevance into a more precise definition that can be implemented with the program analogy. So in this process, we have to make a number of assumptions. This is the first assumption that we make here. Basically, we assume that if a document is more similar to a query than another document. Then the first document will be assumed it will be more relevant than the second one. And this is the basis for ranking documents in this approach. 
1:46 : Again, it's questionable whether this is really the best definition for randoms. As we will see later there are other ways to model randoms. 
1:58 : The basic idea of vectors for base retrieval model is actually very easy to understand. Imagine a high dimensional space where each dimension corresponds to a term. 
2:11 : So here I issue a three dimensional space with three words, programming, library and presidential. So each term here defines one dimension. 
2:24 : Now we can consider vectors in this, three dimensional space. And we're going to assume that all our documents and the query will be placed in this vector space. So for example, on document might be represented by this vector, d1. Now this means this document probably covers library and presidential, but it doesn't really talk about programming. What does this mean in terms of representation of document? That just means we're going to look at our document from the perspective of this vector. We're going to ignore everything else. Basically, what we see here is only the vector root condition of the document. 
3:14 : Of course, the document has all information. For example, the orders of words are [INAUDIBLE] model and that's because we assume that the [INAUDIBLE] of words will [INAUDIBLE]. So with this presentation you can really see d1 simply suggests a [INAUDIBLE] library. Now this is different from another document which might be recommended as a different vector, d2 here. Now in this case, the document that covers programming and library, but it doesn't talk about presidential. So what does this remind you? Well you can probably guess the topic is likely about program language and the library is software lab library. 
3:58 : So this shows that by using this vector space reproduction, we can actually capture the differences between topics of documents. 
4:09 : Now you can also imagine there are other vectors. For example, d3 is pointing into that direction, that might be a presidential program. And in fact we can place all the documents in this vector space. And they will be pointing to all kinds of directions. And similarly, we're going to place our query also in this space, as another vector. 
4:32 : And then we're going to measure the similarity between the query vector and every document vector. So in this case for example, we can easily see d2 seems to be the closest to this query vector. And therefore, d2 will be rendered above others. 
4:51 : So this is basically the main idea of the vector space model. 
4:58 : So to be more precise, vector space model is a framework. In this framework, we make the following assumptions. First, we represent a document and query by a term vector. 
5:18 : So here a term can be any basic concept. For example, a word or a phrase or even n gram of characters. Those are just sequence of characters inside a word. 
5:34 : Each term is assumed that will be defined by one dimension. Therefore n terms in our vocabulary, we define N-dimensional space. 
5:44 : A query vector would consist of a number of elements 
5:49 : corresponding to the weights on different terms. 
5:56 : Each document vector is also similar. It has a number of elements and each value of each element is indicating the weight of the corresponding term. Here, you can see, we assume there are N dimensions. Therefore, they are N elements 
6:15 : each corresponding to the weight on the particular term. 
6:21 : So the relevance in this case will be assumed to be the similarity between the two vectors. 
6:29 : Therefore, our ranking function is also defined as the similarity between the query vector and document vector. 
6:37 : Now if I ask you to write a program to implement this approach in a search engine. 
6:44 : You would realize that this was far from clear. We haven't said a lot of things in detail, therefore it's impossible to actually write the program to implement this. That's why I said, this is a framework. 
6:59 : And this has to be refined in order to actually 
7:04 : suggest a particular ranking function that you can implement on a computer. 
7:10 : So what does this framework not say? Well, it actually hasn't said many things that would be required in order to implement this function. 
7:24 : First, it did not say how we should define or select the basic concepts exactly. 
7:32 : We clearly assume the concepts are orthogonal. Otherwise, there will be redundancy. For example, if two synonyms or somehow distinguish it as two different concepts. Then they would be defining two different dimensions and that would clearly cause redundancy here. Or all the emphasizing of matching this concept, because it would be as if you match the two dimensions when you actually matched one semantic concept. 
8:11 : Secondly, it did not say how we exactly should place documents and the query in this space. Basically that show you some examples of query and document vectors. But where exactly should the vector for a particular document point to? 
8:29 : So this is equivalent to how to define the term weights? How do you compute the lose element values in those vectors? This is a very important question, because term weight in the query vector indicates the importance of term. 
8:48 : So depending on how you assign the weight, you might prefer some terms to be matched over others. 
8:56 : Similarly, the total word in the document is also very meaningful. It indicates how well the term characterizes the document. If you got it wrong then you clearly don't represent this document accurately. 
9:10 : Finally, how to define the similarity measure is also not given. So these questions must be addressed before we can have a operational function that we can actually implement using a program language. 
9:25 : So how do we solve these problems is the main topic of the next lecture. [MUSIC] 
