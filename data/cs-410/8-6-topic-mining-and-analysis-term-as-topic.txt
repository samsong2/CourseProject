8-6-topic-mining-and-analysis-term-as-topic
https://d18ky98rnyall9.cloudfront.net/CS-V-2014-10_TM_13_CSRA_Final.d2a37e8010c511e5bb6bd98da695ecb2/full/540p/index.webm?Expires=1637798400&Signature=Lvm1~amqJcjWq0pBXVOL5SL1iFdr3zL51-BCk~U1m27XNFDmwEZS1wgH-TdCblFEmMDW2dnpfRS5jI5ZyQTs3G5lg8pyvVON0aAnz9tDlihoSLLHpFqIB6hDGDx9kgPAKDaE9GVwCnAtXU8ZQ7qBbefhJOjRzCTI4uELocxqWKU_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
0:00 : [MUSIC] 
0:07 : This lecture is about topic mining and analysis. 
0:12 : We're going to talk about using a term as topic. This is a slide that you have seen in a earlier lecture where we define the task of topic mining and analysis. We also raised the question, how do we exactly define the topic of theta? 
0:31 : So in this lecture, we're going to offer one way to define it, and that's our initial idea. Our idea here is defining a topic simply as a term. 
0:42 : A term can be a word or a phrase. 
0:45 : And in general, we can use these terms to describe topics. So our first thought is just to define a topic as one term. For example, we might have terms like sports, travel, or science, as you see here. Now if we define a topic in this way, we can then analyze the coverage of such topics in each document. Here for example, we might want to discover to what extent document one covers sports. And we found that 30% of the content of document one is about sports. And 12% is about the travel, etc. We might also discover document two does not cover sports at all. So the coverage is zero, etc. 
1:32 : So now, of course, as we discussed in the task definition for topic mining and analysis, we have two tasks. One is to discover the topics. And the second is to analyze coverage. So let's first think about how we can discover topics if we represent each topic by a term. So that means we need to mine k topical terms from a collection. 
2:01 : Now there are, of course, many different ways of doing that. 
2:05 : And we're going to talk about a natural way of doing that, which is also likely effective. So first of all, we're going to parse the text data in the collection to obtain candidate terms. Here candidate terms can be words or phrases. Let's say the simplest solution is to just take each word as a term. These words then become candidate topics. Then we're going to design a scoring function to match how good each term is as a topic. 
2:35 : So how can we design such a function? Well there are many things that we can consider. For example, we can use pure statistics to design such a scoring function. 
2:45 : Intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection. So that would mean we want to favor a frequent term. However, if we simply use the frequency to design the scoring function, then the highest scored terms would be general terms or functional terms like the, etc. Those terms occur very frequently English. 
3:14 : So we also want to avoid having such words on the top so we want to penalize such words. But in general, we would like to favor terms that are fairly frequent but not so frequent. So a particular approach could be based on TF-IDF weighting from retrieval. 
3:35 : And TF stands for term frequency. IDF stands for inverse document frequency. We talked about some of these ideas in the lectures about the discovery of word associations. So these are statistical methods, meaning that the function is defined mostly based on statistics. So the scoring function would be very general. It can be applied to any language, any text. But when we apply such a approach to a particular problem, we might also be able to leverage some domain-specific heuristics. For example, in news we might favor title words actually general. We might want to favor title words because the authors tend to use the title to describe the topic of an article. 
4:27 : If we're dealing with tweets, we could also favor hashtags, which are invented to denote topics. So naturally, hashtags can be good candidates for representing topics. 
4:44 : Anyway, after we have this design scoring function, then we can discover the k topical terms by simply picking k terms with the highest scores. Now, of course, we might encounter situation where the highest scored terms are all very similar. They're semantically similar, or closely related, or even synonyms. So that's not desirable. So we also want to have coverage over all the content in the collection. So we would like to remove redundancy. And one way to do that is to do a greedy algorithm, which is sometimes called a maximal marginal relevance ranking. Basically, the idea is to go down the list based on our scoring function and gradually take terms to collect the k topical terms. The first term, of course, will be picked. When we pick the next term, we're going to look at what terms have already been picked and try to avoid picking a term that's too similar. So while we are considering the ranking of a term in the list, we are also considering the redundancy of the candidate term with respect to the terms that we already picked. 
5:58 : And with some thresholding, then we can get a balance of the redundancy removal and also high score of a term. Okay, so after this that will get k topical terms. And those can be regarded as the topics that we discovered from the connection. Next, let's think about how we're going to compute the topic coverage pi sub ij. 
6:23 : So looking at this picture, we have sports, travel and science and these topics. And now suppose you are give a document. How should we pick out coverage of each topic in the document? 
6:36 : Well, one approach can be to simply count occurrences of these terms. So for example, sports might have occurred four times in this this document and travel occurred twice, etc. And then we can just normalize these counts as our estimate of the coverage probability for each topic. So in general, the formula would be to collect the counts of all the terms that represent the topics. And then simply normalize them so that the coverage of each topic in the document would add to one. 
7:15 : This forms a distribution of the topics for the document to characterize coverage of different topics in the document. Now, as always, when we think about idea for solving problem, we have to ask the question, how good is this one? Or is this the best way of solving problem? 
7:38 : So now let's examine this approach. In general, we have to do some empirical evaluation 
7:46 : by using actual data sets and to see how well it works. 
7:52 : Well, in this case let's take a look at a simple example here. And we have a text document that's about a NBA basketball game. 
8:04 : So in terms of the content, it's about sports. 
8:08 : But if we simply count these words that represent our topics, we will find that the word sports actually did not occur in the article, even though the content is about the sports. 
8:22 : So the count of sports is zero. That means the coverage of sports would be estimated as zero. Now of course, the term science also did not occur in the document and it's estimate is also zero. And that's okay. But sports certainly is not okay because we know the content is about sports. So this estimate has problem. 
8:50 : What's worse, the term travel actually occurred in the document. So when we estimate the coverage of the topic travel, we have got a non-zero count. So its estimated coverage will be non-zero. So this obviously is also not desirable. 
9:08 : So this simple example illustrates some problems of this approach. First, when we count what words belong to to the topic, we also need to consider related words. We can't simply just count the topic word sports. In this case, it did not occur at all. But there are many related words like basketball, game, etc. So we need to count the related words also. The second problem is that a word like star can be actually ambiguous. So here it probably means a basketball star, but we can imagine it might also mean a star on the sky. So in that case, the star might actually suggest, perhaps, a topic of science. 
9:54 : So we need to deal with that as well. Finally, a main restriction of this approach is that we have only one term to describe the topic, so it cannot really describe complicated topics. For example, a very specialized topic in sports would be harder to describe by using just a word or one phrase. We need to use more words. So this example illustrates some general problems with this approach of treating a term as topic. First, it lacks expressive power. Meaning that it can only represent the simple general topics, but it cannot represent the complicated topics that might require more words to describe. 
10:37 : Second, it's incomplete in vocabulary coverage, meaning that the topic itself is only represented as one term. It does not suggest what other terms are related to the topic. Even if we're talking about sports, there are many terms that are related. So it does not allow us to easily count related terms to order, conversion to coverage of this topic. Finally, there is this problem of word sense disintegration. A topical term or related term can be ambiguous. For example, basketball star versus star in the sky. 
11:10 : So in the next lecture, we're going to talk about how to solve the problem with of a topic. [MUSIC] 
