lesson-1-3-text-retrieval-problem
https://d18ky98rnyall9.cloudfront.net/CS-V-2014-7_Text_Retrieval_Problems_CSRA_Final.cc222f30ce5211e4a769493d4d0ab097/full/540p/index.webm?Expires=1639094400&Signature=SxR21n9Z4S8bJaLfYn5iiQfRh~lsNeaQ7tlt7NjEbSQ4CUoXki72RfryGWJ76b1HItE7jsfbUanM-3Xomin2FEDTcfmj6AaLqeLjwlY1yGpCu1-T4U6NVohBccbHQtJvtLD74pQT00ZYvQZi~yJ1iYckFL7c~TtlWhLvP0es0YU_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
0:00 : [MUSIC] This lecture is about the text retrieval problem. 
0:12 : This picture shows our overall plan for lectures. 
0:16 : In the last lecture, we talked about the high level strategies for text access. We talked about push versus pull. 
0:25 : Such engines are the main tools for supporting the pull mode. Starting from this lecture, we're going to talk about the how search engines work in detail. 
0:38 : So first it's about the text retrieval problem. 
0:42 : We're going to talk about the three things in this lecture. First, we define Text Retrieval. Second we're going to make a comparison between Text Retrieval and the related task Database Retrieval. 
0:58 : Finally, we're going to talk about the Document Selection versus Document Ranking as two strategies for responding to a user's query. 
1:09 : So what is Text Retrieval? 
1:12 : It should be a task that's familiar for the most of us because we're using web search engines all the time. 
1:19 : So text retrieval is basically a task where the system would respond to a user's query With relevant documents. Basically, it's for supporting a query 
1:32 : as one way to implement the poll mode of information access. 
1:39 : So the situation is the following. You have a collection of text retrieval documents. These documents could be all the webpages on the web, or all the literature articles in the digital library. Or maybe all the text files in your computer. 
1:58 : A user will typically give a query to the system to express information need. And then, the system would return relevant documents to users. Relevant documents refer to those documents that are useful to the user who typed in the query. 
2:16 : All this task is a phone call that information retrieval. 
2:21 : But literally information retrieval would broadly include the retrieval of other non-textual information as well, for example audio, video, etc. It's worth noting that Text Retrieval is at the core of information retrieval in the sense that other medias such as video can be retrieved by exploiting the companion text data. So for example, current the image search engines actually match a user's query was the companion text data of the image. 
2:59 : This problem is also called search problem. 
3:05 : And the technology is often called the search technology industry. 
3:11 : If you ever take a course in databases it will be useful to pause the lecture at this point and think about the differences between text retrieval and database retrieval. Now these two tasks are similar in many ways. 
3:29 : But, there are some important differences. 
3:33 : So, spend a moment to think about the differences between the two. Think about the data, and the information managed by a search engine versus those that are managed by a database system. 
3:47 : Think about the different between the queries that you typically specify for database system versus queries that are typed in by users in a search engine. 
3:59 : And then finally think about the answers. 
4:02 : What's the difference between the two? Okay, so if we think about the information or data managed by the two systems, we will see that in text retrieval. The data is unstructured, it's free text. But in databases, they are structured data where there is a clear defined schema to tell you this column is the names of people and that column is ages, etc. 
4:31 : The unstructured text is not obvious what are the names of people mentioned in the text. 
4:40 : Because of this difference, we also see that text information tends to be more ambiguous and we talk about that in the processing chapter, whereas in databases. But they don't tend to have where to find the semantics. 
4:58 : The results important difference in the queries, and this is partly due to the difference in the information or data. 
5:07 : So test queries tend to be ambiguous. Whereas in their research, the queries are typically well-defined. Think about a SQL query that would clearly specify what records to be returned. So it has very well-defined semantics. 
5:27 : Keyword queries or electronic queries tend to be incomplete, also in that it doesn't really specify what documents should be retrieved. Whereas complete specification for what should be returned. 
5:47 : And because of these differences, the answers would be also different. Being the case of text retrieval, we're looking for it rather than the documents. 
5:58 : In the database search, we are retrieving records or match records with the sequel query more precisely. 
6:09 : Now in the case of text retrieval, what should be the right answers to the query is not very well specified, as we just discussed. 
6:21 : So it's unclear what should be the right answers to a query. And this has very important consequences, and that is, textual retrieval is an empirically defined problem. 
6:38 : So this is a problem because if it's empirically defined, then we can not mathematically prove one method is better than another method. 
6:52 : That also means we must rely on empirical evaluation involving users to know which method works better. 
7:02 : And that's why we have. You need more than one lectures to cover the issue of evaluation. Because this is very important topic for Sir Jennings. 
7:13 : Without knowing how to evaluate heroism properly, there's no way to tell whether we have got the better or whether one system is better than another. 
7:28 : So now let's look at the problem in a formal way. 
7:32 : So, this slide shows a formal formulation of the text retrieval problem. 
7:37 : First, we have our vocabulary set, which is just a set of words in a language. 
7:44 : Now here, we are considering only one language, but in reality, on the web, there might be multiple natural languages. We have texts that are in all kinds of languages. 
7:57 : But here for simplicity, we just assume that is one kind of language. As the techniques used for retrieving data from multiple languages Are more or less similar to the techniques used for retrieving documents in one end, which although there is important difference, the principle methods are very similar. 
8:21 : Next, we have the query, which is a sequence of words. 
8:26 : And so here, you can see 
8:31 : the query is defined as a sequence of words. Each q sub i is a word in the vocabulary. 
8:42 : A document is defined in the same way, so it's also a sequence of words. And here, d sub ij is also a word in the vocabulary. 
8:52 : Now typically, the documents are much longer than queries. 
8:57 : But there are also cases where the documents may be very short. 
9:04 : So you can think about what might be a example of that case. 
9:09 : I hope you can think of Twitter search. Tweets are very short. 
9:16 : But in general, documents are longer than the queries. 
9:22 : Now, then we have a collection of documents, and this collection can be very large. So think about the web. It could be very large. 
9:36 : And then the goal of text retrieval is you'll find the set of relevant in the documents, which we denote by R'(q), because it depends on the query. And this in general, a subset of all the documents in the collection. 
9:52 : Unfortunately, this set of relevant documents is generally unknown, and user-dependent in the sense that, for the same query typed in by different users, they expect the relevant documents may be different. 
10:09 : The query given to us by the user is only a hint on which document should be in this set. 
10:17 : And indeed, the user is generally unable to specify what exactly should be in this set, especially in the case of web search, where the connection's so large, the user doesn't have complete knowledge about the whole production. 
10:34 : So the best search system can do is to compute an approximation of this relevant document set. So we denote it by R'(q). So formerly, we can see the task is to compute this R'(q) approximation of the relevant documents. So how can we do that? Now imagine if you are now asked to write a program to do this. 
11:08 : What would you do? Now think for a moment. Right, so these are your input. The query, the documents. 
11:20 : And then you are to compute the answers to this query, which is a set of documents that would be useful to the user. 
11:29 : So, how would you solve the problem? Now in general, there are two strategies that we can use. 
11:39 : The first strategy is we do a document selection, and that is, we're going to have a binary classification function, or binary classifier. 
11:49 : That's a function that would take a document and query as input, and then give a zero or one as output to indicate whether this document is relevant to the query or not. 
12:02 : So in this case, you can see the document. 
12:08 : The relevant document is set, is defined as follows. It basically, all the documents that have a value of 1 by this function. 
12:25 : So in this case, you can see the system must have decide if the document is relevant or not. Basically, it has to say whether it's one or zero. And this is called absolute relevance. Basically, it needs to know exactly whether it's going to be useful to the user. 
12:41 : Alternatively, there's another strategy called document ranking. 
12:46 : Now in this case, the system is not going to make a call whether a document is random or not. But rather the system is going to use a real value function, f here. 
12:58 : That would simply give us a value that would indicate which document is more likely relevant. 
13:05 : So it's not going to make a call whether this document is relevant or not. But rather it would say which document is more likely relevant. So this function then can be used to random documents, and then we're going to let the user decide where to stop, when the user looks at the document. So we have a threshold theta here to determine what documents should be in this approximation set. And we're going to assume that all the documents that are ranked above the threshold are in this set, because in effect, these are the documents that we deliver to the user. And theta is a cutoff determined by the user. 
13:56 : So here we've got some collaboration from the user in some sense, because we don't really make a cutoff. And the user kind of helped the system make a cutoff. 
14:08 : So in this case, the system only needs to decide if one document is more likely relevant than another. And that is, it only needs to determine relative relevance, 
14:19 : as opposed to absolute relevance. 
14:22 : Now you can probably already sense that relative relevance would be easier to determine than absolute relevance. Because in the first case, we have to say exactly whether a document is relevant or not. 
14:37 : And it turns out that ranking is indeed generally preferred to document selection. 
14:46 : So let's look at these two strategies in more detail. So this picture shows how it works. So on the left side, we see these documents, and we use the pluses to indicate the relevant documents. So we can see the true relevant documents here consists this set of true relevant documents, consists of these process, these documents. 
15:17 : And with the document selection function, we're going to basically classify them into two groups, relevant documents, and non-relevant ones. Of course, the classified will not be perfect so it will make mistakes. So here we can see, in the approximation of the relevant documents, we have got some number in the documents. 
15:43 : And similarly, there is a relevant document that's misclassified as non-relevant. In the case of document ranking, we can see the system seems like, simply ranks all the documents in the descending order of the scores. And then, we're going to let the user stop wherever the user wants to stop. If the user wants to examine more documents, then the user will scroll down some more and then stop [INAUDIBLE]. But if the user only wants to read a few random documents, the user might stop at the top position. So in this case, the user stops at d4. So in fact, we have delivered these four documents to our user. 
16:33 : So as I said ranking is generally preferred, and one of the reasons is because the classifier in the case of document selection is unlikely accurate. Why? Because the only clue is usually the query. But the query may not be accurate in the sense that it could be overly constrained. 
16:57 : For example, you might expect relevant documents to talk about all these 
17:04 : topics by using specific vocabulary. And as a result, you might match no relevant documents. Because in the collection, no others have discussed the topic using these vocabularies, right? So in this case, we'll see there is this problem of 
17:25 : no relevant documents to return in the case of over-constrained query. 
17:33 : On the other hand, if the query is under-constrained, for example, if the query does not have sufficient descriptive words to find the random documents. You may actually end up having of over delivery, and this when you thought these words my be sufficient to help you find the right documents. But, it turns out they are not sufficient and there are many distractions, documents using similar words. And so, this is a case of over delivery. 
18:08 : Unfortunately, it's very hard to find the right position between these two extremes. 
18:15 : Why? Because whether users looking for the information in general the user does not have a good knowledge about the information to be found. And in that case, the user does not have a good knowledge about what 
18:30 : vocabularies will be used in those relevent documents. So it's very hard for a user to pre-specify the right level of constraints. 
18:44 : Even if the classifier is accurate, we also still want to rend these relevant documents, because they are generally not equally relevant. 
18:56 : Relevance is often a matter of degree. 
18:59 : So we must prioritize these documents for a user to examine. 
19:06 : And note that this prioritization is very important 
19:12 : because a user cannot digest all the content the user generally would have to look at each document sequentially. 
19:21 : And therefore, it would make sense to users with the most relevant documents. And that's what ranking is doing. So for these reasons, ranking is generally preferred. 
19:36 : Now this preference also has a theoretical justification and this is given by the probability ranking principle. 
19:44 : In the end of this lecture, there is reference for this. 
19:49 : This principle says, returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions. 
20:02 : First, the utility of a document (to a user) Is independent of the utility of any other document. 
20:10 : Second, a user would be assumed to browse the results sequentially. Now it's easy to understand why these assumptions are needed in order to justify Site for the ranking strategy. Because if the documents are independent, then we can evaluate the utility of each document that's separate. 
20:36 : And this would allow the computer score for each document independently. And then, we are going to rank these documents based on the scrolls. 
20:45 : The second assumption is to say that the user would indeed follow the rank list. If the user is not going to follow the ranked list, is not going to examine the documents sequentially, then obviously the ordering would not be optimal. 
21:00 : So under these two assumptions, we can theoretically justify the ranking strategy is, in fact, the best that you could do. Now, I've put one question here. Do these two assumptions hold? 
21:18 : I suggest you to pause the lecture, for a moment, to think about this. 
21:27 : Now, can you think of some examples that would suggest these assumptions aren't necessarily true. 
21:44 : Now, if you think for a moment, you may realize none of the assumptions Is actually true. 
21:53 : For example, in the case of independence assumption we might have documents that have similar or exactly the same content. If we look at each of them alone, each is relevant. 
22:07 : But if the user has already seen one of them, we can assume it's generally not very useful for the user to see another similar or duplicated one. 
22:19 : So clearly the utility on the document that is dependent on other documents that the user has seen. 
22:27 : In some other cases you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together. They provide answers to the user's question. 
22:42 : So this is a collective relevance and that also suggests that the value of the document might depend on other documents. 
22:53 : Sequential browsing generally would make sense if you have a ranked list there. 
22:59 : But even if you have a rank list, there is evidence showing that users don't always just go strictly sequentially through the entire list. They sometimes will look at the bottom for example, or skip some. And if you think about the more complicated interfaces that we could possibly use like two dimensional in the phase. Where you can put that additional information on the screen then sequential browsing is a very restricted assumption. 
23:32 : So the point here is that 
23:35 : none of these assumptions is really true but less than that. 
23:41 : But probability ranking principle establishes some solid foundation for 
23:46 : ranking as a primary pattern for search engines. And this has actually been the basis for a lot of research work in information retrieval. And many hours have been designed based on this assumption, 
24:01 : despite that the assumptions aren't necessarily true. And we can address this problem by doing post processing Of a ranked list, for example, to remove redundancy. 
24:20 : So to summarize this lecture, the main points that you can take away are the following. First, text retrieval is an empirically defined Problem. And that means which algorithm is better must be judged by the users. Second, document ranking is generally preferred. And this will help users prioritize examination of search results. 
24:47 : And this is also to bypass the difficulty in determining absolute relevance Because we can get some help from users in determining where to make the cut off, it's more flexible. 
25:01 : So, this further suggests that the main technical challenge in designing a search engine is the design effective ranking function. 
25:10 : In other words, we need to define what is the value of this function F on the query and document pair. 
25:21 : How we design such a function is the main topic in the following lectures. 
25:29 : There are two suggested additional readings. The first is the classical paper on the probability ranking principle. 
25:37 : The second one is a must-read for anyone doing research on information retrieval. It's a classic IR book, which has excellent coverage of the main research and results in early days up to the time when the book was written. Chapter six of this book has an in-depth discussion of the Probability Ranking Principle and Probably for retrieval models in general. [MUSIC] 
